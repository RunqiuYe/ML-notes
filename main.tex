\documentclass[a4paper]{article}
\usepackage{float}

\def\nterm {Spring}
\def\nyear {2025}
\def\ncourse {Machine Learning}

\input{header}

\begin{document}
\maketitle

\tableofcontents

\section{Probability and Statistical Inference}

\subsection{Probability}
\begin{defi}[Types of convergence]
Let $\left\{ X_n \right\}_{n=1}^\infty$ be a sequence of random
variables and $X$ be another random variable. Let 
$F_n$ be the CDF of $X_n$ for each $n \in \N$ and $F$ 
be the CDF of $X$. 
\begin{enumerate}
  \item $X_n$ converges to $X$ \emph{in probability} 
  and write $X_n \convProb X$ if for arbitrary 
  $\epsilon > 0$, 
  \[
  \Prob \left[ \abs{X_n - X} > \epsilon \right] \to 0
  \]
  as $n \to \infty$.

  \item $X_n$ converges to $X$ \emph{in distribution} and 
  write $X_n \convDist X$ if 
  \[
  \lim_{n \to \infty} F_n(t) = F(t)
  \]
  for all $t$ where $F$ is continuous.
  
  \item $X_n$ converges to $X$ in $L^p$ if 
  \[
  \E \left[ \abs{X_n - X}^p \right] \to 0
  \]
  as $n \to \infty$. In particular, say $X_n$ converges to 
  $X$ in \emph{quadratic mean} and write $X_n \convQM X$
  if $X_n$ converges to $X$ in $L^2$.

  \item $X_n$ converges to $X$ \emph{almost surely} 
  and write $X_n \convAS X$ if 
  \[
  \Prob \left[ \lim_{n \to \infty} X_n =  X \right] = 1. 
  \]
\end{enumerate}
\end{defi}

\begin{thm}
The following implication holds:
\begin{enumerate}
  \item If $X_n$ converges to $X$ almost surely, 
  then $X_n$ converges to $X$ in probability. 
  
  \item If $X_n$ converges to $X$ in $L^p$, then 
  $X_n$ converges to $X$ in probability.
\end{enumerate}
\end{thm}

\begin{proof}

\begin{enumerate}
  \item If $X_n$ converges to $X$ almost surely, 
  the set of points $O = \left\{ \omega : \lim_{n \to \infty}
  X_n(\omega) \neq X(\omega) \right\}$ has measure zero.
  Now fix $\epsilon > 0$ and consider the sequence of 
  sets 
  \[
  A_n = \bigcup_{m = n}^\infty \left\{ \abs{X_m - X} > \epsilon \right\}.
  \]
  Note that $A_n \supset A_{n+1}$ for each $n \in \N$ and 
  let $A_\infty = \bigcap_{n=1}^\infty A_n$. 
  Now show $\P[A_\infty] = 0$. If $\omega \notin O$, then 
  $\lim_{n \to \infty} X_n (\omega) = X(\omega)$ and thus 
  $\abs{X_n(\omega) - X(\omega)} < \epsilon$ for some $n \in \N$.
  Therefore, $\omega \notin A_\infty$.
  It follows that $A_\infty \subset O$ and $\P[A_\infty] = 0$. 
  
  By monotone 
  continuity, we have $\lim_{n \to \infty} \P[A_n] = 
  \P[A_\infty]$. It follows that
  \[
  \P \left[ \abs{X_n - X} > \epsilon \right] 
  \leq \P \left[ A_n \right] \to 0 
  \]
  as $n \to \infty$. This completes the proof.

  \item From Chebyshev's inequality, we have 
  \[
  \Prob \left[ \abs{X - X_n} > \epsilon \right] \leq 
  \frac{1}{\epsilon^p} \E [\abs{X - X_n}^p].
  \]
  The claim follows directly.
\end{enumerate}

\end{proof}

\begin{thm}[Central Limit Theorem]
  Let $X_1, \dots, X_n$ be i.i.d. with mean $\mu$ and variance 
  $\sigma^2$. Let $S_n = \frac{1}{n} \sum_{i=1}^n X_i$.
  Then 
  \[
  Z_n = \frac{S_n - \mu}{\sqrt{\var S_n}}
  = \frac{\sqrt{n} \left( S_n - \mu \right)}{\sigma} 
  \convDist Z,
  \]
  where $Z \sim N(0, 1)$. In other words, 
  \[
  \lim_{n \to \infty} \P[Z_n < z] = \Phi(z) 
  = \int_{-\infty}^z \frac{1}{\sqrt{2\pi}} e^{- \frac{x^2}{2}} 
  \; dx.
  \]
  Also write $Z_n \approx N(0, 1)$.
\end{thm}

\subsection{Statistical Inference}

\begin{defi}
  Let $X_1, \dots, X_n$ be $n$ i.i.d. data points from some 
  distribution $F$. A point estimator $\hat{\theta}_n$ 
  of a parameter $\theta$ is some function of 
  $X_1, \dots, X_n$:
  \[
  \hat{\theta}_n = g(X_1, \dots, X_n).
  \]
  The bias of an estimator is defined as 
  \[
  \bias (\hat{\theta}_n)  
  = \E_\theta [\hat{\theta}_n] - \theta.
  \]
  The mean squared error is defined as 
  \[
  \mse = \E_\theta (\hat{\theta}_n - \theta)^2.
  \]
\end{defi}

\begin{defi}
  A point estimator $\hat\theta_n$ of a parameter
  $\theta$ is \emph{consistent} if $\hat{\theta}_n 
  \convProb \theta$.
\end{defi}

\begin{thm}
The MSE can be written as 
\[
\mse = \bias^2 (\hat{\theta}_n) - \var_\theta (\hat{\theta}_n).
\]
\end{thm}

\begin{defi}
  A $1 - \alpha$ interval for a parameter $\theta$ is 
  an interval $C_n = (a, b)$ where 
  $a = a(X_1, \dots, X_n)$ and $b = b(X_1, \dots, X_n)$
  are functions of data such that 
  \[
  \P_\theta[\theta \in C_n] \geq 1 - \alpha 
  \text{ for all $\theta \in \Theta$. }
  \]
  In other word, $(a, b)$ traps $\theta$ with probablity 
  $1 - \alpha$. 
\end{defi}

\textbf{Warning!} In the above definition, 
$C_n$ is random and $\theta$ is fixed.

\end{document}