\documentclass[a4paper]{article}
\usepackage{float}
\usepackage{parskip}

\def\nterm {Spring}
\def\nyear {2025}
\def\ncourse {Machine Learning}

\input{header}

\begin{document}
\maketitle

\tableofcontents

\section{Probability and statistical inference}

\subsection{Probability}
\begin{defi}[Types of convergence]
Let $\left\{ X_n \right\}_{n=1}^\infty$ be a sequence of random
variables and $X$ be another random variable. Let 
$F_n$ be the CDF of $X_n$ for each $n \in \N$ and $F$ 
be the CDF of $X$. 
\begin{enumerate}
  \item $X_n$ converges to $X$ \textbf{in probability} 
  and write $X_n \convProb X$ if for arbitrary 
  $\epsilon > 0$, 
  \[
  \Prob \left[ \abs{X_n - X} > \epsilon \right] \to 0
  \]
  as $n \to \infty$.

  \item $X_n$ converges to $X$ \textbf{in distribution} and 
  write $X_n \convDist X$ if 
  \[
  \lim_{n \to \infty} F_n(t) = F(t)
  \]
  for all $t$ where $F$ is continuous.
  
  \item $X_n$ converges to $X$ in $L^p$ if 
  \[
  \E \left[ \abs{X_n - X}^p \right] \to 0
  \]
  as $n \to \infty$. In particular, say $X_n$ converges to 
  $X$ in \textbf{quadratic mean} and write $X_n \convQM X$
  if $X_n$ converges to $X$ in $L^2$.

  \item $X_n$ converges to $X$ \textbf{almost surely} 
  and write $X_n \convAS X$ if 
  \[
  \Prob \left[ \lim_{n \to \infty} X_n =  X \right] = 1. 
  \]
\end{enumerate}
\end{defi}

\begin{thm}
The following implication holds:
\begin{enumerate}
  \item If $X_n$ converges to $X$ almost surely, 
  then $X_n$ converges to $X$ in probability. 
  
  \item If $X_n$ converges to $X$ in $L^p$, then 
  $X_n$ converges to $X$ in probability.
\end{enumerate}
\end{thm}

\begin{proof}

\begin{enumerate}
  \item If $X_n$ converges to $X$ almost surely, 
  the set of points $O = \left\{ \omega : \lim_{n \to \infty}
  X_n(\omega) \neq X(\omega) \right\}$ has measure zero.
  Now fix $\epsilon > 0$ and consider the sequence of 
  sets 
  \[
  A_n = \bigcup_{m = n}^\infty \left\{ \abs{X_m - X} > \epsilon \right\}.
  \]
  Note that $A_n \supset A_{n+1}$ for each $n \in \N$ and 
  let $A_\infty = \bigcap_{n=1}^\infty A_n$. 
  Now show $\P[A_\infty] = 0$. If $\omega \notin O$, then 
  $\lim_{n \to \infty} X_n (\omega) = X(\omega)$ and thus 
  $\abs{X_n(\omega) - X(\omega)} < \epsilon$ for some $n \in \N$.
  Therefore, $\omega \notin A_\infty$.
  It follows that $A_\infty \subset O$ and $\P[A_\infty] = 0$. 
  
  By monotone 
  continuity, we have $\lim_{n \to \infty} \P[A_n] = 
  \P[A_\infty]$. It follows that
  \[
  \P \left[ \abs{X_n - X} > \epsilon \right] 
  \leq \P \left[ A_n \right] \to 0 
  \]
  as $n \to \infty$. This completes the proof.

  \item From Chebyshev's inequality, we have 
  \[
  \Prob \left[ \abs{X - X_n} > \epsilon \right] \leq 
  \frac{1}{\epsilon^p} \E [\abs{X - X_n}^p].
  \]
  The claim follows directly.
\end{enumerate}

\end{proof}

\begin{thm}[Central Limit Theorem]
  Let $X_1, \dots, X_n$ be i.i.d. with mean $\mu$ and variance 
  $\sigma^2$. Let $S_n = \frac{1}{n} \sum_{i=1}^n X_i$.
  Then 
  \[
  Z_n = \frac{S_n - \mu}{\sqrt{\var S_n}}
  = \frac{\sqrt{n} \left( S_n - \mu \right)}{\sigma} 
  \convDist Z,
  \]
  where $Z \sim \normal(0, 1)$. In other words, 
  \[
  \lim_{n \to \infty} \P[Z_n < z] = \Phi(z) 
  = \int_{-\infty}^z \frac{1}{\sqrt{2\pi}} e^{- \frac{x^2}{2}} 
  \; dx.
  \]
  Also write $Z_n \approx \normal(0, 1)$.
\end{thm}

\subsection{Statistical inference}

\begin{defi}
  Let $X_1, \dots, X_n$ be $n$ i.i.d. data points observed 
  from some distribution $F$ with respect to parameter 
  $\theta$. A point estimator $\hat{\theta}_n$ 
  of the parameter $\theta$ is some function of 
  $X_1, \dots, X_n$:
  \[
  \hat{\theta}_n = g(X_1, \dots, X_n).
  \]
  The bias of an estimator is defined as 
  \[
  \bias (\hat{\theta}_n)  
  = \E_\theta [\hat{\theta}_n] - \theta.
  \]
  The mean squared error is defined as 
  \[
  \mse = \E_\theta (\hat{\theta}_n - \theta)^2.
  \]
\end{defi}

\begin{defi}[Consistent point estimator]
  A point estimator $\hat\theta_n$ of a parameter
  $\theta$ is \textbf{consistent} if $\hat{\theta}_n 
  \convProb \theta$.
\end{defi}

Next we an important relation between bias, variance, 
and MSE. This is a more rigorous way to express the
\textbf{bias-variance tradeoff} of point estimators.

\begin{thm}
The MSE can be written as 
\[
\mse = \bias^2 (\hat{\theta}_n) + \var_\theta (\hat{\theta}_n).
\]
\end{thm}

\begin{proof}
  Let $\bar{\theta}_n = \E_\theta (\hat{\theta}_n)$. 
  Then we have 
  \[
  \begin{aligned}
    \E_\theta(\theta - \hat{\theta}_n)^2 
    &= \E_\theta(\theta - \bar{\theta}_n + \bar{\theta}_n 
    - \hat{\theta}_n)^2 \\
    &= \E_\theta(\theta - \bar{\theta}_n)^2 
    - 2 (\theta - \bar{\theta}_n) \E_\theta (\bar{\theta}_n - 
    \hat{\theta}_n)
    + \E_\theta (\bar{\theta}_n - \theta)^2 \\
    &= (\theta - \bar{\theta}_n)^2 
    + \E_\theta (\bar{\theta}_n - \theta)^2 \\
    &= \bias^2(\hat{\theta}_n) + \var_\theta (\hat{\theta}_n),
  \end{aligned}
  \]
  where we have used the fact that 
  $\E_\theta (\bar{\theta}_n - 
  \hat{\theta}_n) = \bar{\theta}_n - E_\theta(\hat{\theta}_n)
  = \bar{\theta}_n - \bar{\theta}_n = 0$.
\end{proof}

Below is the definition of a confidence set/interval.

\begin{defi}
  A $1 - \alpha$ interval for a parameter $\theta$ is 
  an interval $C_n = (a, b)$ where 
  $a = a(X_1, \dots, X_n)$ and $b = b(X_1, \dots, X_n)$
  are functions of data such that 
  \[
  \P_\theta[\theta \in C_n] \geq 1 - \alpha 
  \text{ for all $\theta \in \Theta$. }
  \]
  In other word, $(a, b)$ traps $\theta$ with probablity 
  $1 - \alpha$. 
\end{defi}

\textbf{Warning!} In the above definition, 
$C_n$ is random and $\theta$ is fixed.

\subsection{PAC learning}

PAC learning is short for Probably Approxinate Correct 
learning, and the setting of PAC learning is as follows:
\begin{itemize}
  \item We have data $x \in \R^d$ and label $y \in \left\{ -1, 
  +1 \right\}$. 
  \item We collect features $x_1, \dots, x_n$ i.i.d. from 
  some distribution $D$. Note that we make no assumption on 
  the distribution $D$ of the features. 
  \item We collect corresponding labels $y_1, \dots, y_n$. 
  \item Assume there exists some true classifier $h^\star$.
  \item Let $\scrh$ be the set of all hypotheses.
\end{itemize}
The goal of PAC learning is $(\epsilon, \delta)$-PAC, 
which is defined as follows. 
\begin{defi}
  An $(\epsilon, \delta)$-PAC learning algorithm
  refers to an algorithm that 
  picks an hypothesis $h \in \scrh$  
  after observing training data 
  $\left\{ x_i, y_i \right\}_{i=1}^n$,
  where the hypothesis $h \in \scrh$ satisfies
  \[
  \err_D (\hat{h}) = 
  \Prob_{x \sim D} \left[ \hat{h}(x) \neq h^\star(x) \right]
  < \epsilon.
  \]
  with probability at least $1 - \delta$, where the probability
  is with respect to the randomness of the training set.
\end{defi}
Putting it more concretely, suppose $g$ is a point estimator
(the algorithm), then we want 
\[
\Prob_{x_i \sim D} \left[ \err_D (g(x_1, \dots, x_n)) < \epsilon 
\right] \geq 1 - \delta.
\]
Through the $\epsilon$-$\delta$ definition of limit, 
it is not hard to  
notice the similarity between this definition 
and \textbf{convergence in probability}.
As we observe more and more data ($n \to \infty$), 
we want the hypothesis 
produce by the algorithm to converge to the true classifier
\textbf{in probability}. This is exactly the definition 
of a \textbf{consistent} point estimator in the previous 
section.

We also say the algorithm is a PAC learner if it uses 
$n$ samples and the running time is at most $\poly
\left( d, \frac{1}{\epsilon}, \log \frac{1}{\delta}, 
\bits(h^\star) \right)$, but in this section we do not 
focus on the runtime of the algorithm.

There are two types of PAC learning -- realizable PAC learning,
in which case $h^\star \in \scrh$, and agnostic PAC learning, 
in which case $h^\star \notin \scrh$. We first discuss 
realizable PAC learning, and we will find out agnostic PAC 
learning is a more general setting but than PAC learning
but a natural extension. 

\subsubsection{Realizable learning}

For realizable PAC learning, we present a simple algorithm:
\begin{algorithm}[Consistent Learner]
Pick any $\hat{h} \in \scrh$ such that $h(x_i) = y_i$ 
for all $1 \leq i \leq n$.
\end{algorithm}
Now we analyze this algorithm in terms of the sample size needed 
to produce a desired hypothesis.

\begin{thm}
  Over the dataset of $n$ i.i.d. samples,
  the consistent learning algorithm produces $\hat{h}$ such 
  that $\err_D(\hat{h}) > \epsilon$ with probability
  at most $\abs{\scrh} e^{-n\epsilon}$. 
\end{thm}
\begin{proof}
  Suppose $h \in \scrh$ is such that 
  $\err_D(h) = \P_{x \sim D} [h(x) \neq h^\star(x)] > \epsilon$.
  For such an $h$ and some data $x_i$, we have 
  \[
  \Prob_{x_i \sim D} [h(x_i) = y_i = h^\star(x_i)] < 1 - \epsilon.
  \]
  Since $\left\{ x_i, y_i \right\}_{i=1}^n$ are i.i.d., we have 
  \[
  \Prob_{x_{1:n} \sim D}[h(x_i) = h^\star(x_i) \text{ for all 
  $1 \leq i \leq n$}] < (1 - \epsilon)^n.
  \]
  Note that our consistent learner do not make any mistake 
  on the training set $\left\{ x_i, y_i \right\}_{i=1}^n$
  \[
  \Prob_{x_{1 : n} \sim D} [h = \hat{h}] < (1 - \epsilon)^n.
  \]
  It follows that 
  \[
  \begin{aligned}
    \Prob_{x_{1 : n} \sim D} [\err_D (\hat{h}) > \epsilon] 
    & \leq \sum_{h \in \scrh: \; \err_D(h) > \epsilon} 
    \Prob[\hat{h} = h]  \\
    & \leq \abs{\scrh} (1 - \epsilon)^n \\
    & \leq \abs{\scrh} e^{-n\epsilon},
  \end{aligned}
  \]
  where in the last step we used the inequality 
  $(1 - u)^n \leq e^{-nu}$. This completes the proof.
\end{proof}

\begin{cor}
  If we want $\P_{x_{1 : n} \sim D} [\err_D (\hat{h}) > \epsilon]
  \leq \delta$, we must have 
  \[
  n \geq \frac{\ln \left( \abs{\scrh} / \delta \right)}{\epsilon}.
  \]
\end{cor}
Technically speaking the implication should be the other direction, 
but this bound for sample size $n$ 
\textbf{gurantees} $(\epsilon, \delta)$-PAC.

To illustrate how we should think of $\abs{\scrh}$, 
we present an example.
\begin{eg}
Consider the binary half-spaces hypotheses: 
\[
\scrh = \left\{ h_w(x) = \sign(\braket{w}{x}) : 
w_i \in \left\{ -1, 1 \right\} \right\}.
\]
In this case $\abs{\scrh} = 2^d$, so $\ln \abs{\scrh} = 
\Theta(d)$. 

We should always think of $\abs{\scrh}$
as exponential with respect to the dimension, so 
$\ln \abs{\scrh}$ is linear with respect to dimension.
\end{eg}

Now we move on to the setting of agnostic leraning.

\subsubsection{Agnostic leraning}

In agnostic learning, again we collect features $x_1, \dots,
x_n \sim D$ i.i.d., and labels $y_1, \dots, y_n$. This forms a 
data set $S_n = \left\{ x_i, y_i \right\}_{i=1}^n$. The 
goal is to use this dataset $S_n$ to produce an hypothesis 
$\hat{h}$ such that 
\[
\reg_{D, \scrh} (\hat{h}) = 
\err_D (\hat{h}) - \min_{h \in \scrh} \err_D (h) < \epsilon
\]
with probability at least $1 - \delta$, where the probability
is with respect to the randomness of the dataset $S_n$.

For this setting, we present an algorithm called the empirical 
loss minimizer (ERM). 
\begin{algorithm}[empirical loss minimizer]
  Choose $\hat{h} \in \scrh$ by minimizing the $\left\{ 0,1
   \right\}$ loss:
  \[
  \hat{h} = \argmin_{h \in \scrh} \sum_{i=1}^n \ind \left\{ 
    h(x_i) = y_i
   \right\}.
  \]
  In a more general setting, suppose $\ell(\cdot, \cdot)$ 
  be any loss function bounded in $[0, 1]$, choose 
  $\hat{h} \in \scrh$ by minimizing the loss:
  \[
  \hat{h} = \argmin_{h \in \scrh} \sum_{i=1}^n \ell(h(x_i), 
  y_i).
  \] 
\end{algorithm}

We next prove a similar relation between the sample 
size and the performance of the hypothesis, evaluated 
in terms of risk. We first present the definition 
of risk.

\begin{defi}[Risk]
  The risk of a hypothesis $h$ is defined as 
  \[
  \risk_D (h) = \E_{x \sim D} [ \ell(h(x), y) ].
  \]
\end{defi}

\begin{thm}
  Over the dataset of $n$ i.i.d. samples, the ERM algorithm
  produces $\hat{h}$ such that 
  \[
  \reg_{D, \scrh} (\hat{h}) 
  = \risk_D (\hat{h}) - \min_{h \in \scrh} 
  \risk_D (h) 
  \leq 2 \epsilon.
  \]
  with probability at least $1 - 2 \abs{\scrh} e^{- 2
  n \epsilon^2}$.
\end{thm}

\begin{proof}
{
  \newcommand{\hopt}{h^{\text{opt}}}
  First we define
  \[
    \hopt = \argmin_{h \in \scrh} \risk_D (h).
  \]
  Note that $\reg_{D, \scrh}(\hopt) = 0$. Define also the 
  estimated risk of hypothesis $h$ for data set $S$:
  \[
  \hat{\risk}_S (h) := \frac{1}{n} \sum_{i=1}^n \ell(h(x_i),
  y_i).
  \]
  Note that $\hat{h} = \argmin_{h \in \scrh} 
  \hat{\risk}_S (h)$ and $\E[\hat{\risk}_S (h)] = 
  \risk_D(h)$, where the expected value is with respect to 
  the randomness of the dataset. Note that 
  \begin{equation*}
    \begin{aligned}
      \reg_{D, \scrh} (\hat{h}) 
      &= \risk_D (\hat{h}) - \risk_D (\hopt) \\
      &= (\risk_D (\hat{h}) - \hat{\risk}_S (\hat{h}))
      + (\hat{\risk}_S (\hat{h}) - \hat{\risk}_S (\hopt)) 
      - (\risk_D (\hopt) - \hat{\risk}_S (\hopt))  \\
      &\leq (\risk_D (\hat{h}) - \hat{\risk}_S (\hat{h}))
      - (\risk_D (\hopt) - \hat{\risk}_S (\hopt)),
    \end{aligned}
    \tag{$*$}
    \label{reg}
  \end{equation*}
  where the inequality is because our algorithm
  always gives
  $\hat{\risk}_S (\hat{h}) - \hat{\risk}_S (\hopt) \leq 0$.

  For any $h \in \scrh$, we have
  \[
  \begin{aligned}
    \hat{\risk}_S (h) - \risk_D(h) 
    &= \frac{1}{n} \sum_{i=1}^n \ell(h(x_i), y_i) 
    - \risk_D (h) \\
    &= \frac{1}{n} \sum_{i=1}^n \left( 
      \ell(h(x_i), y_i) - \E_{x, y \sim D} 
      [\ell(h(x), y)]
     \right).
  \end{aligned}
  \]
  Define now random variable $Z_i := \ell(h(x_i), y_i) - 
  \E_{x, y \sim D} [\ell(h(x), y)]$. Notice that $Z_i$ 
  i.i.d., $\E[Z_i] = 0$, and $\abs{Z_i} \leq 1$. Therefore, 
  we can utilize the \textbf{Hoeffding bound} to get 
  \[
  \Prob \left[ \frac{1}{n} \sum_{i=1}^n Z_i > \epsilon \right]
  \leq e^{- 2 n \epsilon^2} \text{ and }
  \Prob \left[ \frac{1}{n} \sum_{i=1}^n Z_i < -\epsilon \right]
  \leq e^{- 2 n \epsilon^2}
  \]
  This implies that for any fixed $h \in \scrh$,  
  \[
  \Prob \left[ \abs{\hat{\risk}_S(h) - \risk_D(h)} > \epsilon 
  \right] \leq 2 e^{-2 n \epsilon^2}.
  \]
  It follows from the union bound that 
  \[
  \Prob \left[ \exists h \in \scrh: \; 
  \abs{\hat{\risk}_S(h) - \risk_D(h)} > \epsilon \right] 
  \leq 2 \abs{\scrh} e^{- 2 n \epsilon^2}.
  \]
  Notice that if  
  $\abs{\hat{\risk}_S (h) - \risk_D (h)} \leq \epsilon$
  for all $h \in \scrh$, we must have $\reg_{D, \scrh} 
  (\hat{h}) \leq 2 \epsilon$ by inequality \eqref{reg}.
  above. Therefore,
  $\reg_{D, \scrh} (\hat{h}) \leq 2 \epsilon$ with probability at least 
  $1 - 2 \abs{\scrh} e^{- 2 n \epsilon^2}$. 
}
\end{proof}

\begin{cor}
Let $\delta > 0$ be given. Then, 
setting 
$n = \frac{1}{2\epsilon^2} \ln \left( 2 \abs{\scrh} / 
\delta \right)$, we obtain that $\reg_{D, \scrh} (\hat{h})
\leq 2\epsilon$ with probability at least $1 - \delta$.
Therefore, if we want $\reg_{D, \scrh} 
(\hat{h}) \leq \epsilon$ with probability at least $1 - 
\delta$, we must have 
\[
  n \geq \frac{2 \ln 
  \left( 2 \abs{\scrh} / \delta \right)}
  {\epsilon^2}.
\]
\end{cor}

As a comparison between realizable learning and agnostic 
learning, we can see the sample size needed to 
achieve the same $(\epsilon, \delta)$-PAC bound is 
\[
n = \Theta \left( \frac{\ln \left( \abs{\scrh} / \delta
\right)}{\epsilon} \right)
\]
for realizable learning, and 
\[
n = \Theta \left( \frac{\ln(\abs{\scrh} / \delta)}{\epsilon^2} 
\right)
\]
for agnostic learning. Since $\epsilon < 1$, 
we can deduce that agnostic learning needs 
\textbf{more sample} for the estimator to achieve the 
same $(\epsilon, \delta)$-PAC bound.

\subsection{Infinite hypotheses space and VC dimension}

In last section, we discussed PAC learning with finite 
hypotheses space $\scrh$. However, we often need 
to deal with situations where the hypotheses space 
is infinite. Consider the following example.

\begin{eg}
  Consider the binary half space hypotheses: 
  \[
  \scrh = \left\{ h_w(x) = 
  \sign(\braket{w}{x}) : w \in \R^d \right\}.
  \]
  Then, $\abs{\scrh} = \infty$ since each 
  $w \in \R^d$ gives a different decision boundary.
\end{eg}

However, on some fixed dataset $S$, not all of the hypotheses
in the hypotheses space $\scrh$ gives a different prediction. 
This leads to the following definition and theorem, which 
charaterize the ``effective size'' of the hypotheses
space.

\begin{defi}
  Let $S$ be a dataset with $n$ points 
  $\left\{ x_1, \dots, x_n \right\}$. Define 
  $\abs{\scrh(S)}$ as the number of distinct labeling 
  $\left\{ (x_i, y_i) \right\}_{i=1}^n$, where each 
  $y_i = h(x_i)$ for a fixed $h \in \scrh$.
\end{defi}

With this definition, we can derive a similar confidence 
bound for our learned hypotheses.

\begin{thm}
  Define the effective size
  \[
  \scrh [n] = \sup_{x_1, \dots, x_n} 
  \abs{\scrh(\left\{ x_1, \dots, x_n \right\})},
  \]  
  and let $\hat{h}_n$ denote the consistency learner
  from $n$ examples. Then we have 
  \[
  \Prob_{x \sim D} [\err_D(\hat{h}) \geq \epsilon] \leq \delta, 
  \] 
  where 
  \[
  \epsilon = O \left( 
    \frac{\ln \scrh[n] + \ln (1 / \delta)}{n}
   \right).
  \]
\end{thm}

\begin{eg}
  Consider the threshold hypotheses space: 
  \[
  \scrh = \left\{ h_w(x) = 2 \cdot 
  \ind \left\{ x > w \right\} - 1 : w \in \R \right\}.
  \]
  Then it is easy to see that $\scrh[n] = n$. 
  It follows that
  \[
  \epsilon = O\left( 
    \frac{\ln \scrh[n] + \ln (1 / \delta)}{n}
   \right) = O\left( 
    \frac{\ln n + \ln (1 / \delta)}{n}
   \right),
  \]
  which approaches to $0$ as $n \to \infty$.
\end{eg}

Another way to characterize the ``effective size'' of the 
hypotheses space is \textbf{VC dimension}.

\begin{defi}[VC dimension]
  For a hypotheses space $\scrh$, we say 
  $\scrh$ \textbf{shatters} a dataset 
  $\left\{ x_1, \dots, x_n \right\}$ if 
  for any choice of labels 
  $\left\{ -1, 1 \right\}^n$, there exists 
  $h \in \scrh$ such that $h(x_i) = y_i$ 
  for all $1 \leq i \leq n$.
  
  The \textbf{VC dimension} of a hypotheses 
  space $\scrh$ is defined 
  as the largest $n$ such that there exists a dataset $S$  
  of size $n$ such that $\scrh$ shatters $S$.
  Denote this as $\VC(\scrh) = n$.
\end{defi}

The VC dimension of a hypotheses space $\scrh$ is 
related to $\scrh[n]$ by the following lemma by 
Sauer: 

\begin{thm}[Sauer's Lemma]
  For any hypotheses class $\scrh$, we have 
  \[
  \ln \scrh[n] \leq O(\VC(\scrh) \cdot \ln n).
  \]
\end{thm}

Recalling our bound for realizable learning, we get
$(\epsilon, \delta)$-PAC with  
\[
\epsilon = O \left( \frac{\ln \scrh[n] + \ln (1 / \delta)}{n} \right) 
= O \left( \frac{\VC(\scrh) \cdot \log n + \ln (1 / \delta)}{n} \right).
\]




\section{Mixture models and EM}

\subsection{Kullback-Leibler (KL) Divergence}
In this section, we adopt the convention that $0 \log 0 = 0$.

\begin{defi}[KL divergence]
  The Kullback-Leibler (KL) divergence of two  
  distributions $P(X)$ and $Q(X)$ 
  over the outcome space $X$ is defined as follows: 
  \[
  \KL (P \Vert Q) = \sum_{x \in X} P(x) \log \frac{P(x)}{Q(x)}.
  \]
\end{defi}

To understand the significance of KL-divergence better, 
we first discuss some related concepts in information theory,
starting with the definition of \textbf{entropy}.
\begin{defi}
  The entropy of a distribution $P(X)$ 
  is defined as 
  \[
  H(P) = - \sum_{x \in X} P(x) \log P(x)
  \]
\end{defi}

Intuitively, entropy measures how dispersed a probability 
distribution is. For example, a uniform distribution is 
considered to have very high entropy (i.e. a lot of uncertainty), 
whereas a distribution that assigns all its mass on a single 
point is considered to have zero entropy (i.e. no uncertainty). 
Notably, it can be shown that among continuous distributions 
over $\R$, the Gaussian distribution $\normal(\mu, \sigma^2)$ has 
the highest entropy (highest uncertainty) among all possible 
distributions that have the given mean $\mu$ and variance $\sigma^2$.

To further solidify our intuition, we present motivation from 
communication theory. Suppose we want to communicate from a 
source to a destination, and our messages are always 
(a sequence of) discrete symbols over space $X$ 
(for example, $X$ could be letters \{a, b, \dots, z\}). 
We want to construct an encoding scheme for our symbols 
in the form of sequences of binary bits that are transmitted 
over the channel. Further, suppose that in the long run the 
frequency of occurrence of symbols follow a probability 
distribution $P(X)$. This means, in the long run, the fraction 
of times the symbol $x$ gets transmitted is $P(x)$.

A common desire is to construct an encoding scheme such that 
the average number of bits per symbol transmitted remains as 
small as possible. Intuitively, this means we want very 
frequent symbols to be assigned to a bit pattern having a 
small number of bits. Likewise, because we are interested in 
reducing the average number of bits per symbol in the long 
term, it is tolerable for infrequent words to be assigned to 
bit patterns having a large number of bits, since their low 
frequency has little effect on the long term average. The 
encoding scheme can be as complex as we desire, for example, 
a single bit could possibly represent a long sequence of 
multiple symbols (if that specific pattern of symbols is very 
common). The entropy of a probability distribution $P(X)$ is 
its optimal bit rate, i.e., the lowest average bits per 
message that can possibly be achieved if the symbols $x \in X$ 
occur according to $P(X)$. It does not specifically tell us 
how to construct that optimal encoding scheme. It only tells 
us that no encoding can possibly give us a lower long term 
bits per message than $H(P)$.

To see a concrete example, suppose our messages have a 
vocabulary of $K = 32$ symbols, and each symbol has an equal 
probability of transmission in the long term (i.e, uniform 
probability distribution). An encoding scheme that would 
work well for this scenario would be to have $\log_2 K$ bits 
per symbol, and assign each symbol some unique combination 
of the $\log_2 K$ bits. In fact, it turns out that this is the 
most efficient encoding one can come up with for the uniform 
distribution scenario.

It may have occurred to you by now that the long term average 
number of bits per message depends only on the frequency of 
occurrence of symbols. The encoding scheme of scenario A can 
in theory be reused in scenario B with a different set of 
symbols (assume equal vocabulary size for simplicity), 
with the same long term efficiency, as long as the symbols 
of scenario B follow the same probability distribution as the 
symbols of scenario A. It might also have occured to you, 
that reusing the encoding scheme designed to be optimal for 
scenario A, for messages in scenario B having a different 
probability of symbols, will always be suboptimal for 
scenario B. To be clear, we do not need know what the 
specific optimal schemes are in either scenarios. As long 
as we know the distributions of their symbols, we can say 
that the optimal scheme designed for scenario A will be 
suboptimal for scenario B if the distributions are different.

Concretely, if we reuse the optimal scheme designed for a 
scenario having symbol distribution $Q(X)$, into a scenario 
that has symbol distribution $P(X)$, the long term average 
number of bits per symbol achieved is called the cross entropy, 
denoted by $H(P,Q)$:

\begin{defi}
  The cross-entropy of two distributions $P(X)$
  and $Q(X)$ is defined as 
  \[
  H(P, Q) = - \sum_{x \in X} P(X) \log Q(x) 
  \]
\end{defi}

To recap, the entropy $H(P)$ is the best possible long term 
average bits per message (optimal) that can be achived under 
a symbol distribution $P(X)$ by using an encoding scheme 
(possibly unknown) specifically designed for $P(X)$. 
The cross entropy $H(P,Q)$ is the long term average bits 
per message (suboptimal) that results under a symbol 
distribution $P(X)$, by reusing an encoding scheme 
(possibly unknown) designed to be optimal for a scenario 
with symbol distribution $Q(X)$.

Now, KL divergence is the penalty we pay, as measured in 
average number of bits, for using the optimal scheme for 
$Q(X)$, under the scenario where symbols are actually 
distributed as $P(X)$. It is straightforward to see this:
\[
\begin{aligned}
  \KL(P \Vert Q) 
  &= \sum_{x \in X} P(x) \log \frac{P(x)}{Q(x)} \\
  &= \sum_{x \in X} P(x) \log P(x) - \sum_{x \in X} 
  P(x) \log Q(x) \\
  &= H(P, Q) - H(P). 
  \quad \text{ (difference in average number of bits) }
\end{aligned}
\]

If the cross entropy between $P$ and $Q$ is zero 
(and hence $KL(P \Vert Q) = 0$) then it necessarily 
means $P = Q$. In Machine Learning, it is a common 
task to find a distribution $Q$ that is ``close'' to 
another distribution $P$. To achieve this, we use 
$\KL(P \Vert Q)$ to be the loss function to be optimized. 
As we will see in this below, Maximum Likelihood 
Estimation, which is a commonly used optimization objective, 
turns out to be equivalent minimizing KL divergence between 
the training data (i.e. the empirical distribution over the 
data) and the model.

Now we present some useful properties of KL divergence.

\begin{thm}[Non-negativity]
  For any distribution $P$ and $Q$, we have 
  \[
  \KL(P \Vert Q) \geq 0,
  \]
  and $\KL(P \Vert Q) = 0$ if and only if $P = Q$ (almost 
  everywhere).
\end{thm}

\begin{proof}
  By definition,
  \[
    \KL(P \Vert Q) = \sum_{x \in X} P(x) \log \frac{P(x)}{Q(x)} = - \sum_{x \in X} P(x) \log \frac{Q(x)}{P(x)}.
  \] 
  Since $-\log x$ is strictly convex, by Jensen's inequality, we have
  \[
  \begin{aligned}
    \KL(P \Vert Q) = - \sum_{x \in X} P(x) \log \frac{Q(x)}{P(x)}
    \geq -\log \sum_{x \in X} P(x) \frac{Q(x)}{P(x)} = 0.
  \end{aligned}
  \]
  When the equality holds, 
  \[
  \log \frac{Q(x)}{P(x)} = 0
  \]
  almost everywhere. 
  That is, $Q = P$ almost everywhere. 
  This completes the proof.
\end{proof}

\begin{defi}[KL divergence of conditional distribution]
  The KL divergence between 2 conditional distributions 
  $P(X \mid Y)$, $Q(X \mid Y)$ is defined as follows:
  \[
  \KL({P(X \mid Y)} \Vert {Q(X \mid Y)}) = \sum_y P(y) 
  \left( \sum_x P(x \mid y) \log 
  \frac{P(x \mid y)}{Q(x \mid y)} \right).
  \]
  This can be thought of as the expected KL divergence 
  between the corresponding conditional distributions on 
  $x$. That is, between $P(X \mid Y = y)$ and $Q(X \mid Y = y)$, 
  where the expectation is taken over the random $y$.
\end{defi}

\begin{thm}[Chain rule for KL divergence]
The following equality holds:
\[
\KL({P(X, Y)} \Vert {Q(X, Y)}) = 
\KL({P(X)} \Vert {Q(X)}) + \KL({P(Y \mid X)} 
\Vert {Q(Y \mid X)}).
\]
\end{thm}

\begin{proof}
\[
\begin{aligned}
  \text{LHS} &= \sum_x \sum_y P(x,y) \log \frac{P(x,y)}{Q(x,y)} \\
  &= \sum_x \sum_y P(y \mid x) P(x) \left[ \log 
  \frac{P(y \mid x)}{Q(y \mid x)} + \log \frac{P(x)}{Q(x)} \right] \\
  &= \sum_x \sum_y P(y \mid x) P(x) \log 
  \frac{P(y \mid x)}{Q(y \mid x)} + \sum_x P(x) \log 
  \frac{P(x)}{Q(x)} \sum_y P(y \mid x) \\
  &= \sum_x \sum_y P(y \mid x) P(x) \log \frac{P(y \mid x)}
  {Q(y \mid x)} + \sum_x P(x) \log \frac{P(x)}{Q(x)} \\
  &= \KL({P(X)} \Vert {Q(X)}) + \KL({P(Y \mid X)} \Vert {Q(Y \mid X)}) \\
  &= \text{RHS}.
\end{aligned}
\]
\end{proof}

\subsection{The EM Algorithm in General}

Consider a probabilistic model in which we collectively 
denote all of the observed variables by $X$ and all of 
the hidden variables by $Z$. The joint distribution 
$p(X, Z \mid \theta)$ is governed by a set of parameters 
denoted $\theta$. Our goal is to maximize the likelihood 
function that is given by
\[
p(X \mid \theta) = \sum_Z p(X, Z \mid \theta).
\]

Assume direct optimiaztion of $p(X \mid \theta)$ is difficult
but optimiaztion of $p(X, Z \mid \theta)$ is easy.
Introduce distribution $q(Z)$ for the latent vairable $Z$,
we then have 
\[
\ln p(X \mid \theta) = \L(q, \theta) + \KLD{q}{p},
\]
where 
\[
\begin{aligned}
  \L(q, \theta) &= \sum_Z q(Z) \ln \frac{p(X, Z \mid \theta)}{q(Z)}, \\
  \KLD{q}{p} &= - \sum_Z q(Z) \ln \frac{p(Z \mid X, \theta)}{q(Z)}.
\end{aligned}
\]
Note that $\KLD{q}{p}$ is the KL divergence of $q(Z)$
and $p(Z \mid X, \theta)$ so $\KLD{q}{p} \geq 0$
Therefore, $\L(q, \theta) \leq \ln p(X \mid \theta)$. In other word,
$\L(q, \theta)$ is a lower bound for $\ln p(X \mid \theta)$.

Therefore, for the EM algorithm, suppose currently the model 
parameter is $\theta_t$. In the E step, the lower bound 
$\L(q, \theta)$ is maximized with respect to $q(Z)$ while 
holding $\theta$ fixed. Note that $\ln p(X \mid \theta_t)$
is not dependent on $q(Z)$, so the lower bound will 
be maximized when $\KLD{q}{p} = 0$. This is equivalent to
when $q(Z) = p(Z \mid X, \theta)$. Therefore, in the E 
step we simply set
\[
q(Z) = p(Z \mid X, \theta_t).
\]

In the subsequent M step, the distribution $q(Z)$ is fixed
and the lower bound $\L(q, \theta)$ is maximized with 
respect to $\theta$ to give some new model parameter
$\theta_{t + 1}$. Because the distribution $q$ is determined 
using the old parameter values rather than the new values
and is held fixed during the M step, it will not equal the 
new posterior distribution $p(Z \mid X, \theta_{t+1})$,
and hence there will be a nonzero KL divergence.
Substituting the result from E step, we have the following 
expression for the lower bound of M step: 
\[
\begin{aligned}
  \L(q, \theta)
  &= \sum_Z p(Z \mid X, \theta_t) \ln p(X, Z \mid \theta) 
  - \sum_Z p(Z \mid X, \theta_t) \ln p(Z \mid X, \theta_t) \\
  &= Q(\theta_t, \theta) + \const,
\end{aligned}
\]
where the constant is simply the negative entropy of 
the $q$ distribution. Note that the variable $\theta$ 
over which we are optimizing appears only inside the 
logarithm.


\section{Approximate inference} 
A central task in the application of probabilistic models is 
the evaluation of the posterior distribution $p(\Zv|\Xv)$ of the 
latent variables $\Zv$ given the observed (visible) data variables 
$\Xv$, and the evaluation of expectations computed with respect 
to this distribution. For many models of practical interest, 
it will be infeasible 
to evaluate the posterior distribution or indeed to compute 
expectations with respect to this distribution.

In this chapter, we introduce a range of deterministic 
approximation schemes, some of which scale well to large 
applications. These are based on analytical approximations 
to the posterior distribution, for example by assuming that 
it factorizes in a particular way or that it has a specific 
parametric form such as a Gaussian. As such, they can never 
generate exact results, and so their strengths and weaknesses 
are complementary to those of sampling methods.



\end{document}